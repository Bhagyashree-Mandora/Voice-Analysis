<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=640">

    <link rel="stylesheet" href="stylesheets/core.css" media="screen">
    <link rel="stylesheet" href="stylesheets/mobile.css" media="handheld, only screen and (max-device-width:640px)">
    <link rel="stylesheet" href="stylesheets/github-light.css">

    <script type="text/javascript" src="javascripts/modernizr.js"></script>
    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script type="text/javascript" src="javascripts/headsmart.min.js"></script>
    <script type="text/javascript">
      $(document).ready(function () {
        $('#main_content').headsmart()
      })
    </script>
    <title>Voice-analysis by Bhagyashree-Mandora</title>
  </head>

  <body>
    <a id="forkme_banner" href="https://github.com/Bhagyashree-Mandora/Voice-Analysis">View on GitHub</a>
    <div class="shell">

      <header>
        <span class="ribbon-outer">
          <span class="ribbon-inner">
            <h1>Voice-analysis</h1>
            <h2>A tool to predict the gender of a person from their voice</h2>
          </span>
          <span class="left-tail"></span>
          <span class="right-tail"></span>
        </span>
      </header>

      <section id="downloads">
        <span class="inner">
          <a href="https://github.com/Bhagyashree-Mandora/Voice-Analysis/zipball/master" class="zip"><em>download</em> .ZIP</a><a href="https://github.com/Bhagyashree-Mandora/Voice-Analysis/tarball/master" class="tgz"><em>download</em> .TGZ</a>
        </span>
      </section>


      <span class="banner-fix"></span>


      <section id="main_content">
        <h1>
<a id="recognize-gender-and-accent-of-audio-a-study-of-different-algorithms" class="anchor" href="#recognize-gender-and-accent-of-audio-a-study-of-different-algorithms" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Recognize Gender and Accent of audio: A Study of different Algorithms</h1>

<h2>
<a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Motivation</strong>
</h2>

<p>Many  industries  are  moving  towards  the  IVR services, different types of voice inputs and voice based smart systems like virtual assistants. Voice input analysis will contribute to make these systems more intelligent and help in formulating business policies. This project analyses the importance of different acoustic  characteristics  in  prediction  of  gender.  The  method successfully  recognizes  gender  given  an  input  audio  file.  It employs various classification algorithms as a comparison study.It also investigates ways to analyze audio to predict the accent of the associated person.</p>

<h2>
<a id="problem-description" class="anchor" href="#problem-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Problem Description</strong>
</h2>

<p>There has always been a need to identify various characteristics of a person listening to
their voice. This is true in phone conversations or recorded audio from businesses. Nowadays, there is an increasing focus on voice as an input for businesses. Virtual assistants which take voice commands now reside in mobile phones. Technology using voice can now be found in smart systems in cars, shopping complexes, security locks etc. Analyzing audio for such features can help give personalized results for the commands. This can also be used in data analytics to help make intelligent business decisions.</p>

<p>Determining a personâ€™s gender as male or female, based upon a sample of their voice seems to initially be an easy task. Often, the human ear can easily detect the difference between a male or female voice within the first few spoken words. However, designing a computer program to do this turns out to be a bit trickier.</p>

<p>This report describes the design of a computer program to model acoustic analysis of voices and speech for determining gender. The model is constructed using 3,168 recorded samples of male and female voices, speech, and utterances. The samples are processed using acoustic analysis and then applied to an artificial intelligence/machine learning algorithm to learn gender specific traits.</p>

<p>Machine learning algorithms like XGBoost, SVM, Random Forest and Neural Networks and audio processing libraries are employed to predict the outcomes. The resulting program achieves more than 95\% accuracy on the test set.
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/problem%20description.png" alt=""></p>

<h2>
<a id="process" class="anchor" href="#process" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Process</strong>
</h2>

<p>Each voice sample is stored as a .WAV file, which is then preprocessed for acoustic analysis using the specan function from the WarbleR R package. Specan measures 22 acoustic parameters on acoustic signals for which the start and endtimes are provided.The output from the pre-processed WAV files were saved into a CSV file, containing 3168 rows and 21 columns (20 columns for each feature and one label column for the classification of male or female). You can download the pre-processed dataset in CSV format, using the link above</p>

<h2>
<a id="acoustic-properties" class="anchor" href="#acoustic-properties" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Acoustic Properties</strong>
</h2>

<p>The following acoustic properties of each voice are measured:
duration: length of signal</p>

<ul>
<li>meanfreq: mean frequency (in kHz)</li>
<li>sd: standard deviation of frequency</li>
<li>median: median frequency (in kHz)</li>
<li>Q25: first quantile (in kHz)</li>
<li>Q75: third quantile (in kHz)</li>
<li>IQR: interquantile range (in kHz)</li>
<li>skew: skewness (see note in specprop description)</li>
<li>kurt: kurtosis (see note in specprop description)</li>
<li>sp.ent: spectral entropy</li>
<li>sfm: spectral flatness</li>
<li>mode: mode frequency</li>
<li>centroid: frequency centroid (see specprop)</li>
<li>peakf: peak frequency (frequency with highest energy)</li>
<li>meanfun: average of fundamental frequency measured across acoustic signal</li>
<li>minfun: minimum fundamental frequency measured across acoustic signal</li>
<li>maxfun: maximum fundamental frequency measured across acoustic signal</li>
<li>meandom: average of dominant frequency measured across acoustic signal</li>
<li>mindom: minimum of dominant frequency measured across acoustic signal</li>
<li>maxdom: maximum of dominant frequency measured across acoustic signal</li>
<li>dfrange: range of dominant frequency measured across acoustic signal</li>
<li>modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range.</li>
</ul>

<p>The features for duration and peak frequency (peakf) were removed from training. Duration refers to the length of the recording, which for training, is cut off at 20 seconds. Peakf was omitted from calculation due to time and CPU constraints in calculating the value. In this case, all records will have the same value for duration (20) and peak frequency (0).</p>

<h2>
<a id="method" class="anchor" href="#method" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Method</strong>
</h2>

<p>For solving the problem user will input an audio file to the recognizer:</p>

<ul>
<li>Extract audio features from wav file to compute acoustic characteristics</li>
<li>Find importance of acoustic characteristics in prediction</li>
<li>Perform classification using different algorithms</li>
<li>Measure accuracy of algorithm using 10-fold cross validation</li>
<li>Extract scattering coefficient of audio</li>
<li>Predict the accent of speaker</li>
</ul>

<p>Analysis of Importance of Acoustic Characteristics:
1. Hypothesis Test  A proposition was tested with Welch Two Sample t-test method, which is a location test used to compare the medians of two data sets.Here,a proposition that generally a male voice feature is greater or lesser than that of female voice feature is tested. The p-value obtained should be less than 5%.
2. Visual Representation through Boxplot
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/boxplot%20variation%20in%20features.png" alt="">
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/donut.jpg" alt=""></p>

<h2>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Results</strong>
</h2>

<ol>
<li>The model can successfully predict the gender with a good accuracy. Gender classification based on voice characteristics is based primarily on Mean Fundamental Frequency and a value of 142 Hz forms a boundary between male and female voice.
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/decisiontree.jpeg" alt="">
</li>
<li>Varying the Features used  for  Training  and Prediction. Each voice feature was removed from the training and test set one by one and the accuracy was tested on 80% test split data using a Neural Network. A fall in accuracy was noticed when an important feature was removed. However, an increase in accuracy was also obtained when a less important feature was not considered. This may have been due to reduction in noise for prediction.
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/takeout-sorted%20by%20accuracy.jpeg" alt="">
</li>
<li>Five algorithms were used to predict gender and the accuracy obtained is as shown in the chart. SVM showed a comparatively low accuracy. This may have been due to error introduced due to a large number of support vectors found near the separating hyper-plane between male and female values for the feature. Any noisy data leads to a wrong prediction.
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/svm.jpeg" alt="">
<img src="https://github.com/Bhagyashree-Mandora/Voice-Analysis/blob/master/images/Algorithm%20vs%20Accuracy.jpg" alt="">
In the above models, it can be seen how the accuracy of classifying male or female voices was increased by including all available acoustic properties of the voices and speech.Determining a male or female voice does, indeed, utilize more than a simple measurement of average frequency. To demonstrate this, several new voice samples were applied to the model, each using different intonation. For example, the first voice sample used flat or dropping frequency at the end of sentences. A second sample used a rising frequency at the end of sentences. When combined with voice frequency and pitch(i.e., male vs female voice range), this difference in lowering or rising of the voice at the end of a sentence would occasionally signify the difference in a classification of male or female. This is especially true when the male and female voice samples were within a similar, androgynous, frequency range.The above described type of classification makes sense, as male and female speakers will often use changing into nations to express parts of speech. Female voices tend to rise and fall more dramatically than their male counterparts, which might account for this difference</li>
</ol>

<h2>
<a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Conclusion</strong>
</h2>

<p>The prediction of gender is possible using these 21 acoustic characteristics and the importance of each characteristic is also analyzed. The performance of various algorithms is also checked for such a data set.The above model achieves an accuracy of around 95%on the test set. This is a positive achievement, although its certainly not 100%.Its important to keep in mind what the model is actually trained upon. Training data can skew a model from the real world, since the real world often has a much larger variety of data. In the case of voices, there is a large array of both male and female voices that lie within different androgynous zones of frequency and pitch. A dataset that includes a much larger number of samples from the general population would likely train a model that could achieve more accurate results in the wild. After all, a model is only as good as its data.</p>

<h2>
<a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Future Work</strong>
</h2>

<p>Future work includes:</p>

<ul>
<li>Finding the scattering coefficients of the audio file to predict accent</li>
<li>Scattering Coefficients are wavelet transforms with nonlinear operators. For a given language, the scattering coefficients are considerably different for people from different area and different accents. This can be used to differentiate between accents from different audio.</li>
<li>Training machine learning models using Scattering coefficients of audio and predicting accent</li>
<li>Analyzing the results obtained as to how they contribute to accent prediction.</li>
</ul>
      </section>

      <footer>
        <span class="ribbon-outer">
          <span class="ribbon-inner">
            <p>this project by <a href="https://github.com/Bhagyashree-Mandora">Bhagyashree-Mandora</a> can be found on <a href="https://github.com/Bhagyashree-Mandora/Voice-Analysis">GitHub</a></p>
          </span>
          <span class="left-tail"></span>
          <span class="right-tail"></span>
        </span>
        <p>Generated with <a href="https://pages.github.com">GitHub Pages</a> using Merlot</p>
        <span class="octocat"></span>
      </footer>

    </div>

    
  </body>
</html>
